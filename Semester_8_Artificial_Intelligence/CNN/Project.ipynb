{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Convolutional import *\n",
    "from Classification import *\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "TO DO\n",
    "\n",
    "Standartization\n",
    "Batch\n",
    "Batch Normalization\n",
    "Weight-Initilazition Trick\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C_backward_model(dX, caches):\n",
    "    #it needs to check to design\n",
    "    grads={}\n",
    "    L=len(caches) # number of layers in the network\n",
    "    convolutional_cache ,pooling_cache, shape = caches[0]\n",
    "    grads[\"dA\"+str(L+1)]=dX.reshape(shape)\n",
    "    for l in reversed(range(L)):\n",
    "        \n",
    "        convolutional_cache ,pooling_cache, shape = caches[l]\n",
    "        \n",
    "        X = pool_backward(grads[\"dA\"+str(l+2)], pooling_cache)\n",
    "        \n",
    "        grads[\"dA\" + str(l+1)], grads[\"dW\" + str(l+1)], grads[\"db\" + str(l+1)]  = conv_backward(X,convolutional_cache)\n",
    "   \n",
    "    return grads\n",
    "    \n",
    "def C_forward_model(X, Cparameters, hparameters):\n",
    "        \n",
    "    caches=[]\n",
    "    # for this code pool forward and conv_forward are in same layer so 2 layer is actually 1 layer\n",
    "    \n",
    "    L = len(Cparameters) // 2 # number of layers in the network\n",
    "    \n",
    "    for j in range(0,L):\n",
    "        X, convolutional_cache = conv_forward(X,Cparameters['W' + str(j+1)],Cparameters['b' + str(j+1)],hparameters[j][0])\n",
    "        \n",
    "        X, pooling_cache = pool_forward(X,hparameters[j][1])\n",
    "        caches.append([convolutional_cache,pooling_cache,X.shape])\n",
    "    \n",
    "    X = X.reshape((X.shape[0] , np.prod(X.shape[1:]))) #[[[1,2],[3,4]],[[5,6],[7,8]]] -> [[1,2,3,4],[5,6,7,8]]\n",
    "    \n",
    "    return X, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def L_layer_model(X, Y,conv_layers_dims, layers_dims, hparameters,learning_rate = 0.0075, num_iterations = 300, print_cost=True):\n",
    "\n",
    "    costs = []    #keep track of cost\n",
    "    \n",
    "    Cparameters, parameters = initialize_parameters_deep(conv_layers_dims,layers_dims)\n",
    "    #Standardize\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        \n",
    "        #Forward Convolutional\n",
    "        Z, Ccaches= C_forward_model(X,Cparameters,hparameters)\n",
    "        \n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(Z.T, parameters)\n",
    "        \n",
    "        #Compute cost\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        #Backward propagation\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        Cgrads = C_backward_model(grads[\"dA1\"], Ccaches)\n",
    "        \n",
    "        #Update parameters\n",
    "        Cparameters, parameters = update_parameters(Cparameters, parameters, Cgrads, grads, learning_rate)\n",
    "        \n",
    "        #Print the cost every 100 training example\n",
    "        if print_cost and i % 1 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            costs.append(cost)\n",
    "            \n",
    "    #plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return Cparameters, parameters\n",
    "    \n",
    "def initialize_parameters_deep(conv_layer_dims,layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    #Conv\n",
    "    \n",
    "    Cparameters = {}\n",
    "    L = len(conv_layer_dims) \n",
    "    for l in range(1, L+1):\n",
    "        Cparameters['W' + str(l)] = np.random.randn(conv_layer_dims[l-1][0],conv_layer_dims[l-1][1]\n",
    "                                                    ,conv_layer_dims[l-1][2],conv_layer_dims[l-1][3]) \n",
    "        Cparameters['b' + str(l)] = np.zeros((1,1,1,conv_layer_dims[l-1][3]))\n",
    "        assert(Cparameters['W' + str(l)].shape == (conv_layer_dims[l-1][0],conv_layer_dims[l-1][1]\n",
    "                                                    ,conv_layer_dims[l-1][2],conv_layer_dims[l-1][3]))\n",
    "        assert(Cparameters['b' + str(l)].shape == (1,1,1,conv_layer_dims[l-1][3]))\n",
    "    \n",
    "    #Dense\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layer_dims)           # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return Cparameters, parameters\n",
    "\n",
    "\n",
    "\n",
    "def update_parameters(Cparameters,parameters, Cgrads, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    L = len(Cparameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        Cparameters[\"W\" + str(l+1)] = Cparameters[\"W\" + str(l+1)] - learning_rate * Cgrads[\"dW\" + str(l+1)]\n",
    "        Cparameters[\"b\" + str(l+1)] = Cparameters[\"b\" + str(l+1)] - learning_rate * Cgrads[\"db\" + str(l+1)]\n",
    "    \n",
    "            \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return Cparameters, parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[108, 196, 215],\n",
       "         [134, 220, 202],\n",
       "         [ 89, 100, 221],\n",
       "         ...,\n",
       "         [164, 223,  47],\n",
       "         [156,  55, 106],\n",
       "         [ 86, 116, 169]],\n",
       "\n",
       "        [[100, 133, 185],\n",
       "         [ 50, 187,   2],\n",
       "         [ 40, 131, 113],\n",
       "         ...,\n",
       "         [225, 111,  40],\n",
       "         [ 29, 146, 202],\n",
       "         [220, 204, 166]],\n",
       "\n",
       "        [[200, 115, 178],\n",
       "         [204,  98,  71],\n",
       "         [176, 247, 188],\n",
       "         ...,\n",
       "         [124, 101, 142],\n",
       "         [241,  60,  73],\n",
       "         [172, 182, 174]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 44, 254,  13],\n",
       "         [179, 174,  37],\n",
       "         [216, 243,  27],\n",
       "         ...,\n",
       "         [ 15,  98,   1],\n",
       "         [189, 222, 131],\n",
       "         [ 30, 148, 131]],\n",
       "\n",
       "        [[ 69, 166, 121],\n",
       "         [250,  31, 154],\n",
       "         [243,  62,  87],\n",
       "         ...,\n",
       "         [ 76, 187, 227],\n",
       "         [ 30,  69, 132],\n",
       "         [220, 101, 217]],\n",
       "\n",
       "        [[ 92,  63,   4],\n",
       "         [ 88,  51,  61],\n",
       "         [ 41, 178, 254],\n",
       "         ...,\n",
       "         [172,  20, 242],\n",
       "         [ 75,  92, 159],\n",
       "         [229, 131,  17]]],\n",
       "\n",
       "\n",
       "       [[[ 77, 145, 109],\n",
       "         [170, 164, 156],\n",
       "         [148,  42, 128],\n",
       "         ...,\n",
       "         [239,  29, 219],\n",
       "         [210,  31, 249],\n",
       "         [ 78, 224, 204]],\n",
       "\n",
       "        [[240,  10, 124],\n",
       "         [172,  53, 226],\n",
       "         [118, 107, 239],\n",
       "         ...,\n",
       "         [ 27,  39, 134],\n",
       "         [121,  64, 184],\n",
       "         [233,  57, 114]],\n",
       "\n",
       "        [[ 24, 164, 169],\n",
       "         [ 45, 164, 101],\n",
       "         [ 48, 187,  91],\n",
       "         ...,\n",
       "         [ 88, 181, 113],\n",
       "         [138, 187, 153],\n",
       "         [159, 163,  58]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[221,   2, 139],\n",
       "         [ 31, 232,   7],\n",
       "         [ 22, 237, 130],\n",
       "         ...,\n",
       "         [ 53, 124, 159],\n",
       "         [172, 245,   0],\n",
       "         [131, 232,  77]],\n",
       "\n",
       "        [[ 41, 245, 108],\n",
       "         [211, 195,  60],\n",
       "         [251,  15, 203],\n",
       "         ...,\n",
       "         [ 46,  24,  46],\n",
       "         [110, 139,  37],\n",
       "         [166,  77, 199]],\n",
       "\n",
       "        [[ 75, 132,   1],\n",
       "         [ 33, 205,  70],\n",
       "         [208, 194, 219],\n",
       "         ...,\n",
       "         [ 48,  33,  54],\n",
       "         [197,  29, 194],\n",
       "         [169, 230,  94]]]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=np.random.randint(255,size=(2,100,100,3))\n",
    "#A.shape\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparameters = [[{\"pad\" : 2,\"stride\": 1},{\"f\" : 2,\"stride\": 1}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.000165\n",
      "Cost after iteration 1: 0.000000\n"
     ]
    }
   ],
   "source": [
    "Cp,p = L_layer_model(A,np.zeros((1,2)),[[2,2,3,8]],[83232,3,1],hparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
