{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Firt Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install selenium\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\") \n",
    "chrome_path = './chromedriver'\n",
    "driver = webdriver.Chrome(executable_path=chrome_path, options=chrome_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling And Create Huge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Links=[\"https://www.deu.edu.tr/\"]\n",
    "G = nx.Graph()\n",
    "count=0\n",
    "G.add_node(Links[count])\n",
    "try:\n",
    "    while count< len(Links):\n",
    "        try:\n",
    "# =============================================================================\n",
    "#             Acces Website    \n",
    "# =============================================================================\n",
    "            driver.get(Links[count])\n",
    "            elems = driver.find_elements_by_xpath(\"//a[@href]\")\n",
    "        \n",
    "            #Write Website \n",
    "            pageSource = driver.page_source\n",
    "            fileToWrite = open(\"./All_Website_Link/{}.html\".format(count), \"w\",encoding='utf-8')\n",
    "            fileToWrite.write(pageSource)\n",
    "            fileToWrite.close()\n",
    "            \n",
    "            #Find New Link that Inside The Links\n",
    "            for elem in elems:\n",
    "                try:\n",
    "                    link=elem.get_attribute(\"href\")\n",
    "                    if link:# check null\n",
    "                        if \"deu.edu\" in link  and not( #the html must include deu.edu\n",
    "                           \"mailto\" in link or #eleminate social media\n",
    "                           \"whatsapp\" in link or\n",
    "                           \"facebook\" in link or\n",
    "                           \"addtoany\" in link or\n",
    "                           \"twitter\" in link or\n",
    "                           link[-1]=='#' or #eleminate nj\n",
    "                           link==\"javascript:void(0);\" or #eleminate js\n",
    "                           link==\"javascript:void(0)\" or\n",
    "                           '.jpg' in link or #eleminate multimedia\n",
    "                           'css' in link or\n",
    "                           'json' in link or\n",
    "                           '.jpeg' in link or\n",
    "                           '.png' in link or\n",
    "                           '.rar' in link or\n",
    "                           '.zip' in link or\n",
    "                           link.count('/')>5 or #eleminate deep html\n",
    "                           '/feed/' in link or #eleminate not essential page\n",
    "                           'lang=en' in link or #eleminate en page\n",
    "                           'google.' in link or #eleminate big site\n",
    "                           '.pdf' in link or #eleminate file format\n",
    "                           '.doc' in link or\n",
    "                            '#offcanvas' in link or\n",
    "                            '?OGR_NO=' in link or\n",
    "                           '.odt' in link or\n",
    "                           'wp-content' in link or \n",
    "                            '?SEARCH=' in link or\n",
    "                           '/en/' in link or #eleminate en page\n",
    "                           'acikerisim.deu.edu.tr' in link or #high resource page\n",
    "                           'katalog.deu.edu.tr/search?' in link or\n",
    "                            ('debis.deu.edu.tr/ders-katalog/' in link and\n",
    "                             '/2020-2021/tr' not in link) or\n",
    "                           'online.deu.edu.tr' in link or #eleminate login page\n",
    "                           'onlinetip.deu.edu.tr' in link):#chech everything\n",
    "                            if link in Links:\n",
    "                                 G.add_edge(Links[count],link)\n",
    "                            else:\n",
    "                                if 100000>(len(Links)):\n",
    "                                    Links.append(link)\n",
    "                                    G.add_node(link)\n",
    "                                    G.add_edge(Links[count],link)\n",
    "                                else:\n",
    "                                    continue\n",
    "                except Exception:\n",
    "                    continue    \n",
    "        except Exception as e:\n",
    "            print(\"!!!{}. Index and {} Link else went wrong with {}\".format(count,Links[count],e))\n",
    "            continue\n",
    "        finally:\n",
    "            print(\"Completed :%{}\\nRight Now On: {}. Index\\nQue: {}\".format((count/len(Links))*100,count,len(Links)))\n",
    "            count+=1\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write All Crawded Site\n",
    "with open(\"All_Sub_Links.txt\", 'w') as f:\n",
    "    for link in Links:\n",
    "        f.write(\"%s\\n\" % link)\n",
    "f.close()\n",
    "print(len(Links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Second Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install TurkishStemmer\n",
    "#!pip install wordcloud\n",
    "#!pip install textblob\n",
    "#!pip install nltk\n",
    "\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from textblob import TextBlob,Word\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Links=[]\n",
    "f = open(\"All_Sub_Links.txt\", \"r\")\n",
    "Links=f.read().splitlines()\n",
    "f.close()\n",
    "len(Links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RawDataFrame=[]\n",
    "Name=[]\n",
    "for i in os.listdir(\"All_Website_Link/\"):#\n",
    "    file = open(\"All_Website_Link/{}\".format(i), \"r\" ,encoding='utf-8')#\n",
    "    Name.append(\"{}\".format(Links[int(i[:-5])]))\n",
    "    RawDataFrame.append(file.read())\n",
    "    file.close()\n",
    "    \n",
    "#dataframe=pd.DataFrame(RawDataFrame,columns=[\"Body\"])\n",
    "#dataframe['WebPage']=Name\n",
    "#df=dataframe['Body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "df=df.replace('<script([\\S\\s]*?)>([\\S\\s]*?)<\\/script>',' ',regex=True)#script tag\n",
    "df=df.replace('<style([\\S\\s]*?)>([\\S\\s]*?)<\\/style>',' ',regex=True)#style tag\n",
    "df=df.replace('<[a-zA-Z\\/][^>]*>',' ',regex=True)# html tag\n",
    "\n",
    "\n",
    "df=df.replace('[^\\w\\s:)(]','',regex=True)\n",
    "df=df.replace('\\d+','',regex=True)\n",
    "\n",
    "f = open(\"StopWord.txt\", \"r\",encoding='utf-8')\n",
    "sw=f.read().splitlines()\n",
    "f.close()\n",
    "stemmer = TurkishStemmer()\n",
    "\n",
    "df=df.apply(lambda x: \" \".join(stemmer.stem(x) for x in x.split() if x not in sw))\n",
    "df=df.apply(lambda x: TextBlob(x).words)\n",
    "dataframe[\"Body\"]=df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Frequency={}\n",
    "count=0\n",
    "for i in dataframe[\"Body\"]:\n",
    "    print(\"{}. Index was Calculated\".format(count))\n",
    "    count+=1\n",
    "    for j in set(i):\n",
    "        if j in Frequency.keys():\n",
    "            Frequency[j][0]+=i.count(j)\n",
    "            Frequency[j][1]+=1\n",
    "        else:\n",
    "            Frequency[j]=[i.count(j),1]\n",
    "\n",
    "Frequency=sorted(Frequency.items(), key=lambda x: x[1][0],reverse=True)\n",
    "with open(\"Frequency.csv\", 'w' ,encoding='utf-8') as f:\n",
    "    f.write(\"{},{},{}\".format(\"Word\",\"Frequency\",\"Frequency For Page\"))\n",
    "    for i in Frequency:\n",
    "        f.write(\"{},{},{}\\n\".format(i[0],i[1][0],i[1][1]))\n",
    "f.close()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plt Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wordcloud = WordCloud(max_font_size = 50,\n",
    "                     max_words = 100, \n",
    "                     background_color = \"white\").generate(\" \".join(x for x in np.transpose(Frequency[:100])[0]))\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation = \"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "wordcloud.to_file(\"HighFreq_Words.png\");\n",
    "deu_mask = np.array(Image.open(\"deu.png\"))\n",
    "masked_cloud = WordCloud(background_color = \"white\",\n",
    "                     max_words = 1000, \n",
    "                     mask = deu_mask, \n",
    "                     contour_width = 3,\n",
    "                     contour_color = \"firebrick\")\n",
    "\n",
    "masked_cloud.generate(\" \".join(x for x in np.transpose(Frequency[:100])[0]))\n",
    "\n",
    "masked_cloud.to_file(\"deu_mask.png\")\n",
    "\n",
    "plt.figure(figsize = [10,10])\n",
    "plt.imshow(masked_cloud, interpolation = \"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centrality Analysy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install networkx==2.5.1\n",
    "#!pip install decorator==5.0.5\n",
    "betweenness_centrality=nx.betweenness_centrality(G)\n",
    "betweenness_centrality=sorted(betweenness_centrality.items(), key=lambda x: x[1],reverse=True)\n",
    "\n",
    "closeness_centrality=nx.closeness_centrality(G)\n",
    "closeness_centrality=sorted(closeness_centrality.items(), key=lambda x: x[1],reverse=True)\n",
    "\n",
    "degree_centrality=nx.degree_centrality(G)\n",
    "degree_centrality=sorted(degree_centrality.items(), key=lambda x: x[1],reverse=True)\n",
    "#20000^2*log20000+200000*1000000 is high comp so it got small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Centrality Analysy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"betweenness_centrality.csv\", 'w' ,encoding='utf-8') as f:\n",
    "    for i in betweenness_centrality:\n",
    "        f.write(\"{},{}\\n\".format(i[0],i[1]))\n",
    "f.close() \n",
    "\n",
    "with open(\"closeness_centrality.csv\", 'w' ,encoding='utf-8') as f:\n",
    "    for i in closeness_centrality:\n",
    "        f.write(\"{},{}\\n\".format(i[0],i[1]))\n",
    "f.close() \n",
    "\n",
    "with open(\"degree_centrality.csv\", 'w' ,encoding='utf-8') as f:\n",
    "    for i in degree_centrality:\n",
    "        f.write(\"{},{}\\n\".format(i[0],i[1]))\n",
    "f.close() \n",
    "with open(\"centrality.csv\", 'w' ,encoding='utf-8') as f:\n",
    "    f.write(\"{},{},{},{},{},{}\\n\".format(\"WebPage\",\"degree_centrality\",\"WebPage\",\"betweenness_centrality\",\"WebPage\",\"closeness_centrality\"))\n",
    "    i=0\n",
    "    while i<len(degree_centrality):\n",
    "        f.write(\"{},{},{},{},{},{}\\n\".format(degree_centrality[i][0],degree_centrality[i][1],betweenness_centrality[i][0],betweenness_centrality[i][1],closeness_centrality[i][0],closeness_centrality[i][1]))\n",
    "        i+=1\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plt Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colors = ['blue', 'grey', 'lawngreen']\n",
    "betweenness_centrality_p=np.transpose(betweenness_centrality)[1].astype(float)\n",
    "degree_centrality_p=np.transpose(degree_centrality)[1].astype(float)\n",
    "closeness_centrality_p=np.transpose(closeness_centrality)[1].astype(float)\n",
    "\n",
    "vp = plt.violinplot(np.transpose([betweenness_centrality_p,degree_centrality_p,closeness_centrality_p]),\n",
    "                    showmedians=True)\n",
    "\n",
    "plt.xticks([1, 2, 3], ['betweenness_centrality', 'degree_centrality', 'closeness_centrality'])\n",
    "\n",
    "for i in range(len(vp['bodies'])):\n",
    "    vp['bodies'][i].set(facecolor=colors[i])\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('centrality.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Small and Good For Centrality Analysy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limited=[i for i in Links[:3000] if not 'hastane' in i and not 'debis' in i and not 'haber' in i and not 'kutuphane' in i and not 'bid' in i and not 'kisi' in i and not 'engelsiz' in i and not 'tupbebek' in i and not 'calisan' in i and not 'deuzem' in i and not 'katalog' in i and not 'decem' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "import networkx as nx\n",
    "i=0\n",
    "G = nx.Graph()\n",
    "while(i<len(limited)):\n",
    "    if not limited[i] in G.nodes():\n",
    "        G.add_node(limited[i])\n",
    "    i+=1\n",
    "i=0\n",
    "while(i<len(limited)):\n",
    "    html = RawDataFrame[Links.index(limited[i])]\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser',from_encoding=\"iso-8859-1\")\n",
    "    for link in soup.find_all(['a', 'link'], href=True):\n",
    "        if link['href'] in limited:\n",
    "            G.add_edge(link['href'],Links[Links.index(limited[i])])\n",
    "    i+=1\n",
    "    print([len(G.nodes),i])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing Centratily Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as py\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_x = []\n",
    "edge_y = []\n",
    "for edge in G.edges():\n",
    "    x0, y0 = G.nodes[edge[0]]['pos']\n",
    "    x1, y1 = G.nodes[edge[1]]['pos']\n",
    "    edge_x.append(x0)\n",
    "    edge_x.append(x1)\n",
    "    edge_x.append(None)\n",
    "    edge_y.append(y0)\n",
    "    edge_y.append(y1)\n",
    "    edge_y.append(None)\n",
    "\n",
    "edge_trace = go.Scatter(\n",
    "    x=edge_x, y=edge_y,\n",
    "    line=dict(width=0.5, color='#888'),\n",
    "    hoverinfo='none',\n",
    "    mode='lines')\n",
    "\n",
    "node_x = []\n",
    "node_y = []\n",
    "for node in G.nodes():\n",
    "    x, y = G.nodes[node]['pos']\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x, y=node_y,\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    marker=dict(\n",
    "        showscale=True,\n",
    "        colorscale='YlGnBu',\n",
    "        reversescale=True,\n",
    "        color=[],\n",
    "        size=10,\n",
    "        colorbar=dict(\n",
    "            thickness=15,\n",
    "            title='degree_centrality_p',\n",
    "            xanchor='left',\n",
    "            titleside='right'\n",
    "        ),\n",
    "        line_width=2))\n",
    "\n",
    "node_adjacencies = []\n",
    "node_text = []\n",
    "for i,j in degree_centrality:\n",
    "    node_adjacencies.append(j)\n",
    "    node_text.append('{}:'.format(i)+str(j))\n",
    "\n",
    "node_trace.marker.color = node_adjacencies\n",
    "node_trace.text = node_text\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[edge_trace, node_trace],\n",
    "             layout=go.Layout(\n",
    "                title='Network graph made by Suca',\n",
    "                titlefont_size=16,\n",
    "                showlegend=False,\n",
    "                hovermode='closest',\n",
    "                margin=dict(b=20,l=5,r=5,t=40),\n",
    "                annotations=[ dict(\n",
    "                    text=\"\",\n",
    "                    showarrow=False,\n",
    "                    xref=\"paper\", yref=\"paper\",\n",
    "                    x=0.005, y=-0.002 ) ],\n",
    "                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    "                )\n",
    "fig.show()\n",
    "py.plot(fig,filename = 'degree_centrality_p.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_x = []\n",
    "edge_y = []\n",
    "for edge in G.edges():\n",
    "    x0, y0 = G.nodes[edge[0]]['pos']\n",
    "    x1, y1 = G.nodes[edge[1]]['pos']\n",
    "    edge_x.append(x0)\n",
    "    edge_x.append(x1)\n",
    "    edge_x.append(None)\n",
    "    edge_y.append(y0)\n",
    "    edge_y.append(y1)\n",
    "    edge_y.append(None)\n",
    "\n",
    "edge_trace = go.Scatter(\n",
    "    x=edge_x, y=edge_y,\n",
    "    line=dict(width=0.5, color='#888'),\n",
    "    hoverinfo='none',\n",
    "    mode='lines')\n",
    "\n",
    "node_x = []\n",
    "node_y = []\n",
    "for node in G.nodes():\n",
    "    x, y = G.nodes[node]['pos']\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x, y=node_y,\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    marker=dict(\n",
    "        showscale=True,\n",
    "        colorscale='YlGnBu',\n",
    "        reversescale=True,\n",
    "        color=[],\n",
    "        size=10,\n",
    "        colorbar=dict(\n",
    "            thickness=15,\n",
    "            title='closeness_centrality_p',\n",
    "            xanchor='left',\n",
    "            titleside='right'\n",
    "        ),\n",
    "        line_width=2))\n",
    "\n",
    "node_adjacencies = []\n",
    "node_text = []\n",
    "for i,j in closeness_centrality:\n",
    "    node_adjacencies.append(j)\n",
    "    node_text.append('{}:'.format(i)+str(j))\n",
    "\n",
    "node_trace.marker.color = node_adjacencies\n",
    "node_trace.text = node_text\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[edge_trace, node_trace],\n",
    "             layout=go.Layout(\n",
    "                title='Network graph made by Suca',\n",
    "                titlefont_size=16,\n",
    "                showlegend=False,\n",
    "                hovermode='closest',\n",
    "                margin=dict(b=20,l=5,r=5,t=40),\n",
    "                annotations=[ dict(\n",
    "                    text=\"\",\n",
    "                    showarrow=False,\n",
    "                    xref=\"paper\", yref=\"paper\",\n",
    "                    x=0.005, y=-0.002 ) ],\n",
    "                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    "                )\n",
    "fig.show()\n",
    "py.plot(fig,filename = 'closeness_centrality_p.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_x = []\n",
    "edge_y = []\n",
    "for edge in G.edges():\n",
    "    x0, y0 = G.nodes[edge[0]]['pos']\n",
    "    x1, y1 = G.nodes[edge[1]]['pos']\n",
    "    edge_x.append(x0)\n",
    "    edge_x.append(x1)\n",
    "    edge_x.append(None)\n",
    "    edge_y.append(y0)\n",
    "    edge_y.append(y1)\n",
    "    edge_y.append(None)\n",
    "\n",
    "edge_trace = go.Scatter(\n",
    "    x=edge_x, y=edge_y,\n",
    "    line=dict(width=0.5, color='#888'),\n",
    "    hoverinfo='none',\n",
    "    mode='lines')\n",
    "\n",
    "node_x = []\n",
    "node_y = []\n",
    "for node in G.nodes():\n",
    "    x, y = G.nodes[node]['pos']\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x, y=node_y,\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    marker=dict(\n",
    "        showscale=True,\n",
    "        colorscale='YlGnBu',\n",
    "        reversescale=True,\n",
    "        color=[],\n",
    "        size=10,\n",
    "        colorbar=dict(\n",
    "            thickness=15,\n",
    "            title='betweenness_centrality_p',\n",
    "            xanchor='left',\n",
    "            titleside='right'\n",
    "        ),\n",
    "        line_width=2))\n",
    "\n",
    "node_adjacencies = []\n",
    "node_text = []\n",
    "for i,j in betweenness_centrality:\n",
    "    node_adjacencies.append(j)\n",
    "    node_text.append('{}:'.format(i)+str(j))\n",
    "\n",
    "node_trace.marker.color = node_adjacencies\n",
    "node_trace.text = node_text\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[edge_trace, node_trace],\n",
    "             layout=go.Layout(\n",
    "                title='Network graph made by Suca',\n",
    "                titlefont_size=16,\n",
    "                showlegend=False,\n",
    "                hovermode='closest',\n",
    "                margin=dict(b=20,l=5,r=5,t=40),\n",
    "                annotations=[ dict(\n",
    "                    text=\"\",\n",
    "                    showarrow=False,\n",
    "                    xref=\"paper\", yref=\"paper\",\n",
    "                    x=0.005, y=-0.002 ) ],\n",
    "                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    "                )\n",
    "fig.show()\n",
    "py.plot(fig,filename = 'betweenness_centrality_p.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
